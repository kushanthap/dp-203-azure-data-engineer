{
	"name": "07 spark pool trasform data",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "spark9nlosdj",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "2733216f-ff6e-48bc-83cb-426d408ae525"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/3ab7621d-8ee4-445a-8dd2-8b1f6718c4dc/resourceGroups/dp203-9nlosdj/providers/Microsoft.Synapse/workspaces/synapse9nlosdj/bigDataPools/spark9nlosdj",
				"name": "spark9nlosdj",
				"type": "Spark",
				"endpoint": "https://synapse9nlosdj.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark9nlosdj",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"source": [
					"# Transform data by using Spark\n",
					"\n",
					"Apache Spark provides a distributed data processing platform that you can use to perform complex data transformations at scale.\n",
					"\n",
					"\n",
					"## Load source data\n",
					"\n",
					"Let's start by loading some historical sales order data into a dataframe.\n",
					"\n",
					"Review the code in the cell below, which loads the sales order from all of the csv files within the **data** directory. Then click the **&#9655;** button to the left of the cell to run it.\n",
					"\n",
					"> **Note**: The first time you run a cell in a notebook, the Spark pool must be started; which can take several minutes."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"# infer schema\n",
					"root_folder = 'abfss://files@datalake9nlosdj.dfs.core.windows.net/dp-203/Allfiles/labs/06'\n",
					"order_details = spark.read.csv(root_folder+'/data/*.csv', header=True, inferSchema=True)\n",
					"display(order_details.limit(5))"
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"display(order_details.limit(5))"
				],
				"execution_count": 22
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# check schema\r\n",
					"order_details.printSchema()"
				],
				"execution_count": 5
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# custom schema\r\n",
					"from pyspark.sql.types import *\r\n",
					"# from spark.sql.functions import *\r\n",
					"custom_schema = StructType(\r\n",
					"    [\r\n",
					"        StructField('SalesOrderNumber', StringType()),\r\n",
					"        StructField('SalesOrderLineNumber', StringType()),\r\n",
					"        StructField('OrderDate', DateType()),\r\n",
					"        StructField('CustomerName', StringType()),\r\n",
					"        StructField('EmailAddress', StringType()),\r\n",
					"        StructField('Item', StringType()),\r\n",
					"        StructField('Quantity', FloatType()),\r\n",
					"        StructField('UnitPrice', FloatType()),\r\n",
					"        StructField('TaxAmount', FloatType()),\r\n",
					"    ]\r\n",
					")\r\n",
					"\r\n",
					"order_details = spark.read.load(\r\n",
					"    path=root_folder+'/data/*.csv',\r\n",
					"    format='csv',\r\n",
					"    schema=custom_schema,\r\n",
					"    header=True\r\n",
					")\r\n",
					"\r\n",
					"order_details.printSchema()\r\n",
					""
				],
				"execution_count": 15
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Transform the data structure\r\n",
					"\r\n",
					"The source data includes a **CustomerName** field, that contains the customer's first and last name. Let's modify the dataframe to separate this field into separate **FirstName** and **LastName** fields."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"order_details.columns"
				],
				"execution_count": 17
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"# select columns\r\n",
					"order_details_s = order_details.select('OrderDate', 'SalesOrderNumber', 'TaxAmount')\r\n",
					"# OR order_details_s = order_details['SalesOrderNumber', 'TaxAmount']\r\n",
					"display(order_details_s.limit(10))"
				],
				"execution_count": 25
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"# select columns, filter\r\n",
					"order_details_s = order_details.select('SalesOrderNumber', 'TaxAmount').where(order_details['Item'] == 'Mountain-200 Silver, 38')\r\n",
					"display(order_details_s.limit(10))"
				],
				"execution_count": 27
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"# new columns\r\n",
					"from pyspark.sql.functions import split, col\r\n",
					"\r\n",
					"# Create the new FirstName and LastName fields\r\n",
					"transformed_df = order_details.withColumn(\"FirstName\", split(col(\"CustomerName\"), \" \").getItem(0))\r\n",
					"# transformed_df = transformed_df.withColumn(\"LastName\", split(col(\"CustomerName\"), \" \").getItem(1))\r\n",
					"transformed_df = transformed_df.withColumn(\"LastName\", split(transformed_df[\"CustomerName\"], \" \").getItem(1))\r\n",
					"\r\n",
					"# Remove the CustomerName field\r\n",
					"transformed_df = transformed_df.drop(\"EmailAddress\")\r\n",
					"\r\n",
					"display(transformed_df.limit(5))"
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"# group by \r\n",
					"from pyspark.sql.functions import year\r\n",
					"order_details_s = order_details.select('OrderDate', 'TaxAmount').withColumn('OrderYear', year(col('OrderDate'))).groupBy('OrderYear').sum('TaxAmount')\r\n",
					"# OR order_details_s = order_details['SalesOrderNumber', 'TaxAmount']\r\n",
					"display(order_details_s.limit(10))"
				],
				"execution_count": 38
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# see notebook 14 for special functions for JSON"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Save the transformed data\r\n",
					"\r\n",
					"After making the required changes to the data, you can save the results in a supported file format.\r\n",
					"\r\n",
					"> **Note**: Commonly, *Parquet* format is preferred for data files that you will use for further analysis or ingestion into an analytical store. Parquet is a very efficient format that is supported by most large scale data analytics systems. In fact, sometimes your data transformation requirement may simply be to convert data from another format (such as CSV) to Parquet!\r\n",
					"\r\n",
					"Use the following code to save the transformed dataframe in Parquet format (Overwriting the data if it already exists)."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"rel_path = 'abfss://files@datalake9nlosdj.dfs.core.windows.net/spark_data'"
				],
				"execution_count": 8
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"transformed_df.write.mode(\"overwrite\").parquet(rel_path+'/transformed_data/orders.parquet')\r\n",
					"print (\"Transformed data saved!\")"
				],
				"execution_count": 41
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"In the **files** tab (which should still be open above), navigate to the root **files** container and verify that a new folder named **transformed_data** has been created, containing a file named **orders.parquet**. Then return to this notebook."
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Partition data\n",
					"\n",
					"A common way to optimize performance when dealing with large volumes of data is to partition the data files based on one or more field values. This can significant improve performance and make it easier to filter data.\n",
					"\n",
					"Use the following cell to derive new **Year** and **Month** fields and then save the resulting data in Parquet format, partitioned by year and month."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"from pyspark.sql.functions import year, month, col\r\n",
					"\r\n",
					"dated_df = transformed_df.withColumn(\"Year\", year(col(\"OrderDate\"))).withColumn(\"Month\", month(col(\"OrderDate\")))\r\n",
					"display(dated_df.limit(5))\r\n",
					"dated_df.write.partitionBy(\"Year\",\"Month\").mode(\"overwrite\").parquet(rel_path+\"/partitioned_data\")\r\n",
					"print (\"Transformed data saved!\")"
				],
				"execution_count": 42
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"In the **files** tab (which should still be open above), navigate to the root **files** container and verify that a new folder named **partitioned_data** has been created, containing a hierachy of folders in the format **Year=*NNNN*** / **Month=*N***, each containing a .parquet file for the orders placed in the corresponding year and month. Then return to this notebook.\r\n",
					"\r\n",
					"You can read this data into a dataframe from any folder in the hierarchy, using explicit values or wildcards for partitioning fields. For example, use the following code to get the sales orders placed in 2020 for all months."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"# read partitioned data\r\n",
					"\r\n",
					"# all months in 2020\r\n",
					"orders_2020 = spark.read.parquet(rel_path+'/partitioned_data/Year=2020/Month=*')\r\n",
					"display(orders_2020.limit(5))\r\n",
					"\r\n",
					"# all months in 2020 with Month column returned\r\n",
					"orders_2020 = spark.read.parquet(rel_path+'/partitioned_data/Year=2020')\r\n",
					"display(orders_2020.limit(5))\r\n",
					"\r\n",
					"# all months in 2020 with Year and Month column returned\r\n",
					"orders_2020 = spark.read.parquet(rel_path+'/partitioned_data/')\r\n",
					"display(orders_2020.limit(5))\r\n",
					"\r\n",
					""
				],
				"execution_count": 51
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Write to tables"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Temp views"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"dated_df.createOrReplaceTempView('dated_df')\r\n",
					"\r\n",
					"display(spark.sql('select * from dated_df'))"
				],
				"execution_count": 74
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Categlog Tables"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### From Dataframe"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### External Tables\r\n",
					"Specify a path"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# EXTERNAL TABLE\r\n",
					"# from dataframe as parquet\r\n",
					"transformed_df.write.saveAsTable('tblx_orders',mode='overwrite', format='parquet', path=rel_path+'/external_tables/tblx_orders')"
				],
				"execution_count": 54
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# from dataframe as csv\r\n",
					"transformed_df.write.saveAsTable('tblx_orders_csv',mode='overwrite', format='csv', path=rel_path+'/external_tables/tblx_orders_csv')"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# natively with existing files\r\n",
					"spark.catalog.createExternalTable(\r\n",
					"    'tblx_orders2', path=rel_path+'/transformed_data/orders.parquet/', source='parquet'\r\n",
					")"
				],
				"execution_count": 55
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# natively with existing files with partitions\r\n",
					"spark.catalog.createExternalTable(\r\n",
					"    'tblx_orders3', path=rel_path+'/partitioned_data/', source='parquet', partitionBy=\"Year,Month\"\r\n",
					")"
				],
				"execution_count": 68
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Managed (Internal) Tables"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"transformed_df.write.saveAsTable('tbl_orders',mode='overwrite', format='parquet')"
				],
				"execution_count": 5
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"# list databases\r\n",
					"display(spark.catalog.listDatabases())"
				],
				"execution_count": 3
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"# list tables\r\n",
					"display(spark.catalog.listTables())"
				],
				"execution_count": 10
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"df = spark.sql(\"SELECT * FROM tblx_orders3\")\r\n",
					"display(df.limit(10))"
				],
				"execution_count": 69
			},
			{
				"cell_type": "markdown",
				"source": [
					"## SQL"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\r\n",
					"\r\n",
					"SELECT * FROM dated_df\r\n",
					"WHERE Year = 2021\r\n",
					"    AND Month = 1"
				],
				"execution_count": 80
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\r\n",
					"\r\n",
					"DROP TABLE transformed_orders;\r\n",
					"DROP TABLE sales_orders;"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\r\n",
					"\r\n",
					"-- Catelog table - EXTERNAL\r\n",
					"CREATE TABLE tbl_orders_sql\r\n",
					"USING PARQUET\r\n",
					"LOCATION 'abfss://files@datalake9nlosdj.dfs.core.windows.net/spark_data/transformed_data/orders.parquet'"
				],
				"execution_count": 9
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					}
				},
				"source": [
					"%%sql\r\n",
					"-- Catelog table - Managed or External with empty folder\r\n",
					"%%sql\r\n",
					"\r\n",
					"CREATE TABLE Tbl_ManagedSalesOrders\r\n",
					"(\r\n",
					"    Orderid INT NOT NULL,\r\n",
					"    OrderDate TIMESTAMP NOT NULL,\r\n",
					"    CustomerName STRING,\r\n",
					"    SalesTotal FLOAT NOT NULL\r\n",
					")\r\n",
					"USING PARQUET"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql \r\n",
					"\r\n",
					"-- Querying data in other Lakehouse DBs\r\n",
					"\r\n",
					"USE RetailDB;\r\n",
					"\r\n",
					"SELECT * from Product;"
				],
				"execution_count": 7
			}
		]
	}
}